{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0eec72f",
   "metadata": {},
   "source": [
    "# Graph Optimization with torch.fx\n",
    "---\n",
    "This notebook demonstrates how to implement custom optimization passes in PyTorch using `torch.fx`.\n",
    "We will:\n",
    "- Fuse `Linear → BatchNorm → ReLU` into a single module\n",
    "- Remove redundant operations (e.g., cancelling consecutive transposes)\n",
    "- Rewrite control flow into more graph-friendly operations\n",
    "- Compare correctness and performance (execution time, memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4199e47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA version: 12.4\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "Using device: cuda\n",
      "CPU matmul time: 0.03013134002685547\n",
      "GPU matmul time: 0.06492304801940918\n"
     ]
    }
   ],
   "source": [
    "#SETUP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fx as fx\n",
    "import time\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "\n",
    "\n",
    "# Select device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Quick PyTorch check\n",
    "x_cpu = torch.randn(2000, 2000)\n",
    "x_gpu = x_cpu.to(device)\n",
    "\n",
    "\n",
    "# CPU matmul\n",
    "t0 = time.time()\n",
    "_ = x_cpu @ x_cpu\n",
    "print(\"CPU matmul time:\", time.time() - t0)\n",
    "\n",
    "\n",
    "# GPU matmul\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    _ = x_gpu @ x_gpu\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"GPU matmul time:\", time.time() - t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea9b8a-223d-4837-b43d-c83854b2bea5",
   "metadata": {},
   "source": [
    "## Define a toy model with `Linear → BatchNorm → ReLU`\n",
    "\n",
    "We begin by defining a lightweight feed-forward neural network (`ToyModel`) that captures the canonical `Linear → BatchNorm → ReLU → Linear` computation pattern.  \n",
    "This serves as a controlled testbed for our custom optimization passes. The intermediate composition of linear, normalization, and activation layers is a typical target for graph-level fusion, as it introduces multiple operator boundaries that can be collapsed into a single composite operation without altering functional semantics.  \n",
    "\n",
    "By setting the model to evaluation mode (`.eval()`), we ensure that batch normalization uses its stored statistics rather than minibatch statistics, yielding deterministic behavior suitable for graph transformations and benchmarking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd24403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(32, 64)\n",
    "        self.bn = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.head = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return self.head(x)\n",
    "model = ToyModel().to(device).eval()  \n",
    "x = torch.randn(16, 32).to(device)\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e42e34-b5a9-4a14-840d-2ca5e6ef5ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All BatchNorm layers confirmed to be in eval mode.\n"
     ]
    }
   ],
   "source": [
    "def assert_bn_eval(model: nn.Module):\n",
    "    \"\"\"Ensure all BatchNorm layers are in eval mode before fusion.\"\"\"\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "            if m.training:\n",
    "                raise RuntimeError(\n",
    "                    f\"BatchNorm layer '{name}' must be in eval() mode before fusion.\"\n",
    "                )\n",
    "    print(\"All BatchNorm layers confirmed to be in eval mode.\")\n",
    "assert_bn_eval(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f761cb62-4375-499f-829c-e1b1e6df6eba",
   "metadata": {},
   "source": [
    "## Helper function to fold BatchNorm into Linear\n",
    "\n",
    "This utility function implements **batch normalization folding** into a preceding linear layer’s parameters, a common graph-level optimization for inference.  \n",
    "\n",
    "Given a linear layer (`Linear`) and a batch normalization layer (`BatchNorm`), we analytically compute the equivalent weight (`W_fold`) and bias (`b_fold`) that integrate the normalization and affine transformation directly into the linear layer. This eliminates the need for a separate `BatchNorm` during forward execution, reducing operator overhead and memory footprint.  \n",
    "\n",
    "Key steps include:  \n",
    "1. Extracting the linear layer’s original weights and biases.  \n",
    "2. Accounting for the batch normalization’s affine parameters (`gamma`, `beta`) if they exist.  \n",
    "3. Computing the scaling factor based on the running variance and epsilon.  \n",
    "4. Applying the folding transformation to both weights and bias to preserve functional equivalence.  \n",
    "\n",
    "This approach preserves the model’s output while enabling subsequent graph-level fusion and simplification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e436afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedLinearReLU(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, with_relu=True):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.act = nn.ReLU() if with_relu else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.linear(x))\n",
    "\n",
    "\n",
    "def fold_bn_into_linear_params(linear: nn.Linear, bn: nn.BatchNorm1d):\n",
    "    \"\"\"Fold BatchNorm1d parameters into a preceding Linear layer.\"\"\"\n",
    "    W = linear.weight\n",
    "    b = linear.bias if linear.bias is not None else torch.zeros(W.size(0), device=W.device)\n",
    "\n",
    "    gamma = bn.weight if bn.affine else torch.ones(W.size(0), device=W.device)\n",
    "    beta = bn.bias if bn.affine else torch.zeros(W.size(0), device=W.device)\n",
    "    mean = bn.running_mean\n",
    "    var = bn.running_var\n",
    "    eps = bn.eps\n",
    "\n",
    "    scale = gamma / torch.sqrt(var + eps)\n",
    "    W_fused = W * scale.unsqueeze(1)\n",
    "    b_fused = beta + (b - mean) * scale\n",
    "\n",
    "    return W_fused, b_fused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa807b54-5127-47cb-9722-21118c6cc29b",
   "metadata": {},
   "source": [
    "## Function for Custom FX Fusion Pass: Linear → BatchNorm → ReLU\n",
    "\n",
    "This cell defines a transformation pass on a PyTorch fx.GraphModule that detects the pattern: Linear → BatchNorm (eval mode) → [optional ReLU] and replaces it with a single fused operator.\n",
    "\n",
    "**STEP 1: Pattern Matching:**\n",
    "Iterate through graph nodes and check for a Linear module.\n",
    "Verify it is followed by a BatchNorm module in eval() mode.\n",
    "Optionally detect a ReLU (either as a module or function call) immediately after BatchNorm.\n",
    "\n",
    "**STEP 2: Parameter Folding:**\n",
    "Use fold_bn_into_linear_params to mathematically fold BatchNorm parameters into the Linear layer’s weights and biases.\n",
    "This eliminates the runtime BatchNorm computation.\n",
    "\n",
    "**STEP 3: Fused Module Creation:**\n",
    "Construct a new FusedLinearReLU module.\n",
    "If a ReLU is present, attach it inside the fused module; otherwise, use an identity mapping.\n",
    "\n",
    "**STEP 4: Graph Rewriting:**\n",
    "Insert the fused module node into the graph.\n",
    "Redirect all uses of the old BatchNorm/ReLU outputs to the fused node.\n",
    "Erase the original Linear, BatchNorm, and (if present) ReLU nodes.\n",
    "\n",
    "**STEP 5: Graph Validation and Recompile:**\n",
    "Run graph.lint() to ensure graph consistency.\n",
    "Call gm.recompile() so the updated module reflects the optimized graph.\n",
    "\n",
    "This pass enables static graph optimizations that reduce operator count, improve inference latency, and simplify downstream compilation or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c374d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_linear_bn_relu_fx(gm: fx.GraphModule) -> fx.GraphModule:\n",
    "    graph = gm.graph\n",
    "    counter = 0\n",
    "\n",
    "    for node in list(graph.nodes):\n",
    "        if node.op != \"call_module\":\n",
    "            continue\n",
    "\n",
    "        # Check Linear\n",
    "        try:\n",
    "            linear_mod = gm.get_submodule(node.target)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not isinstance(linear_mod, nn.Linear):\n",
    "            continue\n",
    "\n",
    "        linear_users = list(node.users)\n",
    "        if len(linear_users) != 1:\n",
    "            continue\n",
    "\n",
    "        # Check BatchNorm\n",
    "        bn_node = linear_users[0]\n",
    "        if bn_node.op != \"call_module\":\n",
    "            continue\n",
    "        try:\n",
    "            bn_mod = gm.get_submodule(bn_node.target)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not isinstance(bn_mod, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "            continue\n",
    "        if getattr(bn_mod, \"training\", False):\n",
    "            continue\n",
    "\n",
    "        # Optional ReLU\n",
    "        bn_users = list(bn_node.users)\n",
    "        relu_node, is_relu = None, False\n",
    "        if len(bn_users) == 1:\n",
    "            candidate = bn_users[0]\n",
    "            if candidate.op == \"call_module\":\n",
    "                try:\n",
    "                    relu_mod = gm.get_submodule(candidate.target)\n",
    "                    if isinstance(relu_mod, nn.ReLU):\n",
    "                        relu_node, is_relu = candidate, True\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif candidate.op == \"call_function\" and candidate.target in (torch.relu, F.relu):\n",
    "                relu_node, is_relu = candidate, True\n",
    "\n",
    "        # Fold BN into Linear\n",
    "        Wf, bf = fold_bn_into_linear_params(linear_mod, bn_mod)\n",
    "        fused_linear = nn.Linear(linear_mod.in_features, linear_mod.out_features, bias=True)\n",
    "        fused_linear.weight.data.copy_(Wf)\n",
    "        fused_linear.bias.data.copy_(bf)\n",
    "\n",
    "        fused_mod = FusedLinearReLU(fused_linear, with_relu=is_relu)\n",
    "        fused_name = f\"fused_{counter}_{linear_mod._get_name()}_{bn_mod._get_name()}\"\n",
    "        counter += 1\n",
    "        gm.add_submodule(fused_name, fused_mod)\n",
    "\n",
    "        # Insert fused node\n",
    "        with graph.inserting_before(node):\n",
    "            fused_node = graph.call_module(fused_name, args=(node.args[0],))\n",
    "\n",
    "        # Replace old nodes\n",
    "        last_node = relu_node if relu_node is not None else bn_node\n",
    "        last_node.replace_all_uses_with(fused_node)\n",
    "\n",
    "        # Erase old nodes\n",
    "        if relu_node is not None:\n",
    "            graph.erase_node(relu_node)\n",
    "        graph.erase_node(bn_node)\n",
    "        graph.erase_node(node)\n",
    "\n",
    "        print(f\"Fused nodes: {node.name} -> {bn_node.name}\" +\n",
    "              (f\" -> {relu_node.name}\" if is_relu else \"\") +\n",
    "              f\" as {fused_name}\")\n",
    "\n",
    "    graph.lint()\n",
    "    gm.recompile()\n",
    "    return gm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e62ee3-1bd6-4176-9d33-903d293b5d18",
   "metadata": {},
   "source": [
    "## Compare correctness between original and fused models\n",
    "This cell performs functional correctness verification of the fused graph transformation.\n",
    "\n",
    "compare_models evaluates two modules (a and b) on the same input tensor x and checks element-wise equivalence within specified absolute (atol) and relative (rtol) tolerances. It reports the maximum absolute difference to provide insight into numerical deviations introduced by the fusion process.\n",
    "\n",
    "fx.symbolic_trace generates a GraphModule representation of the original ToyModel, which is then passed to fuse_linear_bn_relu_fx to produce a fused variant.\n",
    "\n",
    "Finally, compare_models confirms that the fused model produces outputs numerically equivalent to the original, ensuring that the Linear → BatchNorm → ReLU fusion preserves the model’s functional semantics.\n",
    "\n",
    "This step is critical in validating that graph-level optimizations maintain correctness before any performance benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d1eee9-6bfd-42bb-abea-cc9098f1f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(a: nn.Module, b: nn.Module, x: torch.Tensor, atol=1e-6, rtol=1e-5):\n",
    "    a.eval()\n",
    "    b.eval()\n",
    "    with torch.no_grad():\n",
    "        out_a = a(x)\n",
    "        out_b = b(x)\n",
    "    max_diff = (out_a - out_b).abs().max().item()\n",
    "    print(f\"max_abs_diff: {max_diff:.6e}\")\n",
    "    assert torch.allclose(out_a, out_b, atol=atol, rtol=rtol), \"Mismatch detected!\"\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdb984e2-3fdb-405d-9b07-ce0f0d30ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test fusion by tracing a model, applying fusion, visualizing graphs before/after, printing fused FX code, and verifying correctness.\n",
    "\n",
    "def test_fusion(model: nn.Module, input_shape=(4, 32)):\n",
    "    # Ensure BN layers are in eval\n",
    "    assert_bn_eval(model)\n",
    "\n",
    "    # FX trace\n",
    "    traced = fx.symbolic_trace(model.eval())\n",
    "\n",
    "    # Nodes before fusion\n",
    "    print(\"\\nNodes BEFORE fusion:\")\n",
    "    for n in traced.graph.nodes:\n",
    "        print(f\"{n.name:20} | {n.op:12} | {n.target}\")\n",
    "\n",
    "    # Fusion\n",
    "    fused = fuse_linear_bn_relu_fx(traced)\n",
    "\n",
    "    # Nodes after fusion\n",
    "    print(\"\\nNodes AFTER fusion:\")\n",
    "    for n in fused.graph.nodes:\n",
    "        print(f\"{n.name:20} | {n.op:12} | {n.target}\")\n",
    "\n",
    "    # Fused FX code\n",
    "    print(\"\\nFused Graph Code:\\n\", fused.code)\n",
    "\n",
    "    # Test correctness\n",
    "    x = torch.randn(*input_shape)\n",
    "    compare_models(model, fused, x)\n",
    "    print(\"Fusion correctness verified\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e052201c-13e2-4acd-a046-6e73a820af5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All BatchNorm layers confirmed to be in eval mode.\n",
      "\n",
      "Nodes BEFORE fusion:\n",
      "x                    | placeholder  | x\n",
      "fc                   | call_module  | fc\n",
      "bn                   | call_module  | bn\n",
      "relu                 | call_module  | relu\n",
      "head                 | call_module  | head\n",
      "output               | output       | output\n",
      "Fused nodes: fc -> bn -> relu as fused_0_Linear_BatchNorm1d\n",
      "\n",
      "Nodes AFTER fusion:\n",
      "x                    | placeholder  | x\n",
      "fused_0_linear_batch_norm1d | call_module  | fused_0_Linear_BatchNorm1d\n",
      "head                 | call_module  | head\n",
      "output               | output       | output\n",
      "\n",
      "Fused Graph Code:\n",
      " \n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    fused_0_linear_batch_norm1d = self.fused_0_Linear_BatchNorm1d(x);  x = None\n",
      "    head = self.head(fused_0_linear_batch_norm1d);  fused_0_linear_batch_norm1d = None\n",
      "    return head\n",
      "    \n",
      "max_abs_diff: 1.005828e-07\n",
      "Fusion correctness verified\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = ToyModel().eval()\n",
    "    test_fusion(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cfe1ad-a682-4c6e-bde8-91a837a9e7b0",
   "metadata": {},
   "source": [
    "## Benchmarking Performance\n",
    "\n",
    "This cell implements a **performance benchmarking routine** to quantify the impact of graph-level fusion on inference efficiency.  \n",
    "\n",
    "`benchmark_model` measures both **execution latency** and **peak memory usage** for a given model and input shape:  \n",
    "1. Warm-up iterations stabilize runtime performance and account for JIT or caching effects.  \n",
    "2. For CUDA devices, `torch.cuda.Event` is used to record high-precision timing, and `torch.cuda.max_memory_allocated` captures peak memory consumption.  \n",
    "3. For CPU execution, `time.perf_counter` provides wall-clock timing, though memory profiling is not included.  \n",
    "\n",
    "By comparing the original and fused models, we can evaluate how operator fusion reduces runtime overhead and memory footprint, validating the practical benefits of the custom `Linear → BatchNorm → ReLU` fusion pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ee039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_model(model: nn.Module, input_shape=(1024, 32), device=None, warmup=10, iters=100):\n",
    "    \"\"\"\n",
    "    Benchmark a model's inference performance.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to benchmark (should already be in eval mode).\n",
    "        input_shape (tuple): Shape of a single input batch.\n",
    "        device (torch.device or None): Target device (defaults to 'cuda' if available, else 'cpu').\n",
    "        warmup (int): Number of warm-up iterations.\n",
    "        iters (int): Number of timed iterations.\n",
    "\n",
    "    Returns:\n",
    "        dict with 'latency_ms' (per iteration) and 'peak_mem_mb' (if CUDA).\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    x = torch.randn(*input_shape, device=device)\n",
    "\n",
    "    # Ensure determinism\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # Warm-up phase\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(x)\n",
    "\n",
    "    # CUDA benchmark\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        torch.cuda.synchronize(device)\n",
    "        start_event.record()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(iters):\n",
    "                _ = model(x)\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize(device)\n",
    "\n",
    "        elapsed_ms = start_event.elapsed_time(end_event) / iters\n",
    "        peak_mem_mb = torch.cuda.max_memory_allocated(device) / (1024 ** 2)\n",
    "\n",
    "        result = {\"latency_ms\": elapsed_ms, \"peak_mem_mb\": peak_mem_mb}\n",
    "\n",
    "    # CPU benchmark\n",
    "    else:\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(iters):\n",
    "                _ = model(x)\n",
    "        end = time.perf_counter()\n",
    "        elapsed_ms = (end - start) * 1000 / iters\n",
    "\n",
    "        result = {\"latency_ms\": elapsed_ms, \"peak_mem_mb\": None}\n",
    "\n",
    "    print(f\"Benchmark on {device}: {result}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "396de581-2ab8-46d8-bf55-24644b6464aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused nodes: fc -> bn -> relu as fused_0_Linear_BatchNorm1d\n",
      "\n",
      "--- Benchmark Original ---\n",
      "Benchmark on cuda: {'latency_ms': 0.047531838417053225, 'peak_mem_mb': 41.5556640625}\n",
      "\n",
      "--- Benchmark Fused ---\n",
      "Benchmark on cuda: {'latency_ms': 0.0357478404045105, 'peak_mem_mb': 41.56396484375}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = ToyModel().eval()\n",
    "    fused_model = fuse_linear_bn_relu_fx(fx.symbolic_trace(model))\n",
    "\n",
    "    # Benchmark both\n",
    "    print(\"\\n--- Benchmark Original ---\")\n",
    "    bench_orig = benchmark_model(model, input_shape=(1024, 32))\n",
    "\n",
    "    print(\"\\n--- Benchmark Fused ---\")\n",
    "    bench_fused = benchmark_model(fused_model, input_shape=(1024, 32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bd7fc6f-aebc-4c28-8e16-4aa9e140dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_peak_memory(model: nn.Module, input_shape=(1024, 32), device=None):\n",
    "    \"\"\"\n",
    "    Measure peak GPU memory usage (MB) for a single forward pass.\n",
    "    Returns None if not running on CUDA.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device.type != \"cuda\":\n",
    "        print(\"Peak memory measurement only supported on CUDA.\")\n",
    "        return None\n",
    "\n",
    "    model = model.to(device).eval()\n",
    "    x = torch.randn(*input_shape, device=device)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "    torch.cuda.synchronize(device)\n",
    "\n",
    "    peak_mem_mb = torch.cuda.max_memory_allocated(device) / (1024 ** 2)\n",
    "    return peak_mem_mb\n",
    "\n",
    "\n",
    "def compare_peak_memory(orig_model: nn.Module, fused_model: nn.Module, input_shape=(1024, 32)):\n",
    "    \"\"\"\n",
    "    Compare peak GPU memory usage between original and fused models.\n",
    "    On CPU, reports N/A.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    mem_orig = measure_peak_memory(orig_model, input_shape, device)\n",
    "    mem_fused = measure_peak_memory(fused_model, input_shape, device)\n",
    "\n",
    "    print(\"\\nPeak GPU Memory Usage\")\n",
    "    if device.type == \"cuda\":\n",
    "        print(f\"Original: {mem_orig:.2f} MB\")\n",
    "        print(f\"Fused   : {mem_fused:.2f} MB\")\n",
    "        reduction = (mem_orig - mem_fused) / mem_orig * 100 if mem_orig else 0.0\n",
    "        print(f\"Reduction: {reduction:.2f}%\")\n",
    "    else:\n",
    "        print(\"CUDA not available — memory usage measurement skipped.\")\n",
    "\n",
    "    return {\"orig_mb\": mem_orig, \"fused_mb\": mem_fused}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "277d042d-fb41-4c8d-8392-e6b6bf437bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused nodes: fc -> bn -> relu as fused_0_Linear_BatchNorm1d\n",
      "\n",
      "Peak GPU Memory Usage\n",
      "Original: 41.58 MB\n",
      "Fused   : 41.59 MB\n",
      "Reduction: -0.02%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = ToyModel().eval()\n",
    "    traced = fx.symbolic_trace(model)\n",
    "    fused_model = fuse_linear_bn_relu_fx(traced)\n",
    "\n",
    "    compare_peak_memory(model, fused_model, input_shape=(1024, 32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7108066c-8c25-4067-8d35-ffde3ee5d9e4",
   "metadata": {},
   "source": [
    "## Redundant Operation Removal (Transpose Cancellation)\n",
    "\n",
    "This cell implements a **graph-level optimization pass to eliminate redundant transpositions**.  \n",
    "\n",
    "The function `remove_redundant_transposes` traverses the FX `GraphModule` and identifies consecutive `transpose` operations that **cancel each other out** (i.e., transposing the same two dimensions twice). When such patterns are detected:  \n",
    "1. The output of the second transpose is replaced directly with the input of the first, effectively bypassing both operations.  \n",
    "2. Both transpose nodes are removed from the graph, reducing computational overhead.  \n",
    "\n",
    "By performing this transformation, the graph becomes **more efficient and streamlined**, avoiding unnecessary memory permutations while preserving functional equivalence. The call to `graph.lint()` and `gm.recompile()` ensures the modified FX graph is consistent and executable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6298d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_redundant_transposes(gm: fx.GraphModule, verbose: bool = True) -> fx.GraphModule:\n",
    "    \"\"\"\n",
    "    Graph-level optimization pass to remove redundant transpose ops.\n",
    "    Looks for consecutive transpose calls with identical (dim0, dim1) that cancel out.\n",
    "    \"\"\"\n",
    "    graph = gm.graph\n",
    "    removed = 0\n",
    "\n",
    "    for node in list(graph.nodes):\n",
    "        if node.op != \"call_function\" or node.target != torch.transpose:\n",
    "            continue\n",
    "\n",
    "        users = list(node.users)\n",
    "        if len(users) != 1:\n",
    "            continue\n",
    "        next_node = users[0]\n",
    "\n",
    "        if next_node.op == \"call_function\" and next_node.target == torch.transpose:\n",
    "            # both should have the same dims\n",
    "            if (len(node.args) >= 3 and len(next_node.args) >= 3):\n",
    "                dim0, dim1 = node.args[1], node.args[2]\n",
    "                dim0_next, dim1_next = next_node.args[1], next_node.args[2]\n",
    "\n",
    "                if dim0 == dim0_next and dim1 == dim1_next:\n",
    "                    # cancel: replace all uses of next_node with the original input to first transpose\n",
    "                    orig_input = node.args[0]\n",
    "                    next_node.replace_all_uses_with(orig_input)\n",
    "\n",
    "                    # Erase the two transposes\n",
    "                    graph.erase_node(next_node)\n",
    "                    graph.erase_node(node)\n",
    "\n",
    "                    removed += 2\n",
    "                    if verbose:\n",
    "                        print(f\"Removed redundant transpose pair on dims ({dim0}, {dim1})\")\n",
    "\n",
    "    if removed > 0:\n",
    "        graph.lint()\n",
    "        gm.recompile()\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"No redundant transposes found.\")\n",
    "\n",
    "    return gm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44276226-b766-48a2-8e0b-f8cb6f21fe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization:\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %transpose : [num_users=1] = call_method[target=transpose](args = (%x, 1, 2), kwargs = {})\n",
      "    %transpose_1 : [num_users=1] = call_method[target=transpose](args = (%transpose, 1, 2), kwargs = {})\n",
      "    return transpose_1\n",
      "No redundant transposes found.\n",
      "\n",
      "After optimization:\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %transpose : [num_users=1] = call_method[target=transpose](args = (%x, 1, 2), kwargs = {})\n",
      "    %transpose_1 : [num_users=1] = call_method[target=transpose](args = (%transpose, 1, 2), kwargs = {})\n",
      "    return transpose_1\n"
     ]
    }
   ],
   "source": [
    "class TransposeToy(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Redundant double transpose\n",
    "        return x.transpose(1, 2).transpose(1, 2)\n",
    "\n",
    "model = TransposeToy()\n",
    "traced = fx.symbolic_trace(model)\n",
    "\n",
    "print(\"Before optimization:\")\n",
    "print(traced.graph)\n",
    "\n",
    "optimized = remove_redundant_transposes(traced)\n",
    "\n",
    "print(\"\\nAfter optimization:\")\n",
    "print(optimized.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "688b2794-3952-4912-aa1b-ab8eb3aaf513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fusion_and_optimization(model: nn.Module, input_shape=(4, 32)):\n",
    "    \"\"\"\n",
    "    Run fusion (Linear+BN+ReLU) and redundant transpose elimination,\n",
    "    then compare against the original model for correctness.\n",
    "    \"\"\"\n",
    "    # Ensure BatchNorm layers are frozen\n",
    "    assert_bn_eval(model)\n",
    "\n",
    "    # Trace\n",
    "    traced = fx.symbolic_trace(model.eval())\n",
    "\n",
    "    # --- Step 1: Fusion ---\n",
    "    fused = fuse_linear_bn_relu_fx(traced)\n",
    "\n",
    "    # --- Step 2: Remove redundant transposes ---\n",
    "    optimized = remove_redundant_transposes(fused)\n",
    "\n",
    "    # --- Step 3: Print final graph ---\n",
    "    print(\"\\nOptimized Graph Code:\\n\", optimized.code)\n",
    "\n",
    "    # --- Step 4: Correctness test ---\n",
    "    x = torch.randn(*input_shape)\n",
    "    compare_models(model, optimized, x)\n",
    "    print(\"Fusion + Optimization correctness verified\\n\")\n",
    "\n",
    "    return optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3b60fa9-6eff-42cb-a649-d79e941fb9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All BatchNorm layers confirmed to be in eval mode.\n",
      "Fused nodes: fc -> bn -> relu as fused_0_Linear_BatchNorm1d\n",
      "No redundant transposes found.\n",
      "\n",
      "Optimized Graph Code:\n",
      " \n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    fused_0_linear_batch_norm1d = self.fused_0_Linear_BatchNorm1d(x);  x = None\n",
      "    head = self.head(fused_0_linear_batch_norm1d);  fused_0_linear_batch_norm1d = None\n",
      "    return head\n",
      "    \n",
      "max_abs_diff: 7.450581e-08\n",
      "Fusion + Optimization correctness verified\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = ToyModel().eval()\n",
    "    optimized_model = test_fusion_and_optimization(model, input_shape=(4, 32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b11a5-3af4-440f-bc2d-f9e7a13ff87b",
   "metadata": {},
   "source": [
    "## Control Flow Rewriting\n",
    "Masked assignments (e.g., y[mask] = value) are not compatible with torch.fx tracing because in-place item assignment on Proxy objects is unsupported. To make the model graph-friendly:\\\n",
    "Replace in-place masked assignments with torch.where.\n",
    "Use torch.full_like to create the replacement tensor safely, preserving shape, dtype, and device, even when x is a Proxy.\n",
    "Compatible with symbolic tracing (torch.fx.symbolic_trace).\n",
    "Preserves functional correctness: outputs match the original in-place masked assignment.\n",
    "Enables downstream graph-level optimizations, fusions, or export.\n",
    "Functional equivalence is tested by comparing the output of the original module against the rewritten version.\n",
    "The FX graph confirms that the in-place assignment has been replaced with a single torch.where operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88dc50b2-d55c-4a28-be62-955c78e71e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_abs_diff: 0.000000e+00\n",
      "Outputs match between original and rewritten version\n",
      "\n",
      "FX Graph of rewritten module:\n",
      "graph():\n",
      "    %x : [num_users=2] = placeholder[target=x]\n",
      "    %mask : [num_users=1] = placeholder[target=mask]\n",
      "    %full_like : [num_users=1] = call_function[target=torch.full_like](args = (%x, 0.0), kwargs = {})\n",
      "    %where : [num_users=1] = call_function[target=torch.where](args = (%mask, %full_like, %x), kwargs = {})\n",
      "    return where\n"
     ]
    }
   ],
   "source": [
    "class OriginalMaskedAssign(nn.Module):\n",
    "    def forward(self, x, mask):\n",
    "        # In-place masked assignment (not FX-traceable)\n",
    "        y = x.clone()\n",
    "        y[mask] = 0.0\n",
    "        return y\n",
    "\n",
    "\n",
    "class MaskedAssignWhere(nn.Module):\n",
    "    def forward(self, x, mask):\n",
    "        # Graph-friendly version: use torch.where\n",
    "        replacement = torch.full_like(x, 0.0)\n",
    "        return torch.where(mask, replacement, x)\n",
    "\n",
    "\n",
    "def test_masked_assign_equivalence():\n",
    "    x = torch.randn(4, 5)\n",
    "    mask = x > 0  # boolean mask\n",
    "\n",
    "    m_orig = OriginalMaskedAssign()\n",
    "    m_rewritten = MaskedAssignWhere()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y1 = m_orig(x, mask)\n",
    "        y2 = m_rewritten(x, mask)\n",
    "\n",
    "    # Functional correctness check\n",
    "    max_diff = (y1 - y2).abs().max().item()\n",
    "    print(f\"max_abs_diff: {max_diff:.6e}\")\n",
    "    assert torch.allclose(y1, y2), \"Mismatch detected!\"\n",
    "    print(\"Outputs match between original and rewritten version\")\n",
    "\n",
    "    # FX trace of the rewritten version\n",
    "    traced = fx.symbolic_trace(m_rewritten)\n",
    "    print(\"\\nFX Graph of rewritten module:\")\n",
    "    print(traced.graph)\n",
    "\n",
    "\n",
    "# --- Run---\n",
    "if __name__ == \"__main__\":\n",
    "    test_masked_assign_equivalence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c503e910-26af-4359-b3a7-2b0bd3a6e4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fx_opt)",
   "language": "python",
   "name": "fx_opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
